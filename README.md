# Paper Reading List

## Table of Contents
1. [Memory](#memory)
2. [survey](#survey)
3. [InteractVGen](#interactvgen)
4. [Architecture](#architecture)
5. [Accelerate/Real-time](#accelerate-real-time)
6. [MM Understanding](#mm-understanding)
7. [Consistency](#consistency)
8. [Interpretability](#interpretability)
9. [Evaluation](#evaluation)
10. [self-supervised](#self-supervised)
11. [3D/4D](#3d-4d)
12. [Cache](#cache)
13. [Data](#data)
14. [Audio](#audio)
15. [Audio&Video Gen](#audio&video-gen)

## Memory
* [SLOWFAST-VGEN: SLOW-FAST LEARNING FOR ACTION-DRIVEN LONG VIDEO GENERATION](https://arxiv.org/pdf/2410.23277)
* [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](https://arxiv.org/abs/2506.18903)
* [Video World Models with Long-term Spatial Memory](https://spmem.github.io/)
* [Packing Input Frame Context in Next-Frame Prediction Models f...eo Generation](https://lllyasviel.github.io/frame_pack_gitpage/)
* [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](https://v-mem.github.io/)
* [WORLDMEM: Long-term Consistent World Simulation with Memory](https://xizaoqu.github.io/worldmem/)
* [History-Guided Video Diffusion](https://www.boyuan.space/history-guidance/)
* [Mixture of Contexts for Long Video Generation](https://primecai.github.io/moc/)

## survey
* [A Survey of Interactive Generative Video](https://arxiv.org/pdf/2504.21853)
* [Genie3 - Simulate The World [Exclusive Interview]](https://www.youtube.com/watch?v=ekgvWeHidJs)
* [Position: Interactive Generative Video as Next-Generation Game Engine](https://arxiv.org/abs/2506.17210)

## InteractVGen
* [GameGen-X: Interactive Open-World Game Video Generation.](https://arxiv.org/abs/2411.00769)
* [Interactive Video Generation for Video Games](https://arxiv.org/abs/2405.15111)
* [Layered Neural Atlases for Consistent Scene Generation](https://arxiv.org/abs/2409.11511)
* [GameDreamer: Generative Gaming Level Design](https://arxiv.org/abs/2404.06001)
* [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)
* [DIAMOND: Diffusion-based Interactive World Modeling](https://arxiv.org/abs/2405.12399)
* [MineWorld: An Open-Source Minecraft-like World Model](https://arxiv.org/abs/2411.00955)
* [Vid2World: Crafting Video Diffusion Models to Interactive World Models](https://knightnemo.github.io/vid2world/)
* [Yume: An Interactive World Generation Model](https://github.com/stdstu12/YUME)
* [From Slow Bidirectional to Fast Autoregressive Video Diffusion Models](https://github.com/tianweiy/CausVid)
* [LongLive: Real-time Interactive Long Video Generation](https://github.com/NVlabs/LongLive?tab=readme-ov-file)

## Architecture
* [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)
* [Diffusion Transformer](https://arxiv.org/abs/2212.09748)
* [Flow Matching for Generative Modeling](https://arxiv.org/abs/2210.02747)
* [Rectified Flow: A Unified Framework for Generative Modeling](https://arxiv.org/abs/2209.03003)
* [Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/abs/2202.00512)
* [Consistency Models](https://arxiv.org/abs/2303.01469)

## Accelerate/Real-time
* [StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation](https://arxiv.org/pdf/2312.12491)

## MM Understanding
* [InternVideo2: Scaling Video Foundation Models for Multimodal Understanding](https://arxiv.org/abs/2403.15377)
* [VideoChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models](https://arxiv.org/abs/2306.05424)
* [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)

## Consistency
* [Temporal Consistency for Video Diffusion Models](https://arxiv.org/abs/2307.04725)
* [Consistent Video Generation with Diffusion Models](https://arxiv.org/abs/2407.14187)
* [Long-Term Temporal Consistency in Video Generation](https://arxiv.org/abs/2409.18602)
* [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://github.com/Vchitect/LongVie#%EF%B8%8F-control-signal-extraction)

## Interpretability
* [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2403.06058)
* [Diffusion Models and the Manifold Hypothesis](https://arxiv.org/abs/2401.01887)

## Evaluation
* [Video Generation Benchmarks: A Survey](https://arxiv.org/abs/2406.11175)
* [VBench: Comprehensive Benchmark Suite for Video Generation Models](https://arxiv.org/abs/2311.17982)
* [EvalCrafter: Benchmarking Video Generation](https://arxiv.org/abs/2403.07656)

## self-supervised
* [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447)
* [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
* [WavLM: Large-Scale Self-Supervised Pre-training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900)

## 3D/4D
* [3D Gaussian Splatting for Real-Time Radiance Field Rendering](https://arxiv.org/abs/2308.04079)
* [4D Gaussian Splatting for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2310.08528)
* [LGM: Large Gaussian Models for 3D Generation](https://arxiv.org/abs/2402.12563)

## Cache
* [KV Cache Compression in Transformers](https://arxiv.org/abs/2402.04623)
* [Speculative Decoding](https://arxiv.org/abs/2211.17192)
* [PagedAttention](https://arxiv.org/abs/2309.06180)

## Data
* [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://nju-3dv.github.io/projects/SpatialVID/)
* [Sekai: A Video Dataset towards World Exploration](https://lixsp11.github.io/sekai-project/)
* [Dynamic Camera Poses and Where to Find Them](https://research.nvidia.com/labs/dir/dynpose-100k/)

## Audio
* [DIFF-FOLEY: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2306.17203)
* [FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds](https://github.com/open-mmlab/FoleyCrafter)

## audio&video-gen
* [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](https://github.com/character-ai/Ovi)
* [LTX-2](https://github.com/Lightricks/LTX-2)
